{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcff370-0029-46e6-84dd-17c778c60557",
   "metadata": {},
   "source": [
    "# Wide-and-Deep ML: Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3fb5e-4eb3-4b5a-82a7-013a45f44b35",
   "metadata": {},
   "source": [
    "In this notebook, we train and evaluate the wide-and-deep collaborative filtering recommender using features engineered in the prior notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c77f893-fe4a-4d8c-80ac-5022cbe929c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b5512-2c94-4d4e-8931-ed224577607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization, StringLookup\n",
    "import math\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de284318-710d-4e29-94eb-5dcb51950847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories to save the model's training progress, output, and save files\n",
    "CHECKPOINT_PATH = './tmp/model_checkpoint'\n",
    "EXPORT_PATH = './tmp/model_export'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9589b06-7fa9-4a07-b29f-5d1e5e8e80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "\n",
    "# enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d74265-4364-4b29-af7d-ea35b4c09633",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "\n",
    "### 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c4caa2-7ce2-46d8-97cd-dd2db6028dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "train_df = pd.read_csv('../data/user_movie_interaction_train.csv')\n",
    "val_df = pd.read_csv('../data/user_movie_interaction_val.csv')\n",
    "test_df = pd.read_csv('../data/user_movie_interaction_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dca892be-a93b-4199-a952-2c2a01df6380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_movie_rating</th>\n",
       "      <th>user_all_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23695</td>\n",
       "      <td>442</td>\n",
       "      <td>51662</td>\n",
       "      <td>0.4</td>\n",
       "      <td>300 (2007)</td>\n",
       "      <td>action fantasy war imax</td>\n",
       "      <td>0.721622</td>\n",
       "      <td>fantasy sci-fi mystery animation documentary w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37754</td>\n",
       "      <td>417</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Robin Hood: Prince of Thieves (1991)</td>\n",
       "      <td>adventure drama</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>fantasy sci-fi musical horror mystery western ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18178</td>\n",
       "      <td>394</td>\n",
       "      <td>45499</td>\n",
       "      <td>0.5</td>\n",
       "      <td>X-Men: The Last Stand (2006)</td>\n",
       "      <td>action sci-fi thriller</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>sci-fi drama children thriller western film-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33268</td>\n",
       "      <td>271</td>\n",
       "      <td>60609</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Death Note (2006)</td>\n",
       "      <td>adventure crime drama horror mystery</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>sci-fi drama children thriller western film-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47465</td>\n",
       "      <td>489</td>\n",
       "      <td>3301</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Whole Nine Yards, The (2000)</td>\n",
       "      <td>comedy crime</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>sci-fi drama children thriller western film-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  userId  movieId  rating                                 title  \\\n",
       "0       23695     442    51662     0.4                            300 (2007)   \n",
       "1       37754     417     1027     0.4  Robin Hood: Prince of Thieves (1991)   \n",
       "2       18178     394    45499     0.5          X-Men: The Last Stand (2006)   \n",
       "3       33268     271    60609     0.9                     Death Note (2006)   \n",
       "4       47465     489     3301     0.6          Whole Nine Yards, The (2000)   \n",
       "\n",
       "                                 genres  avg_movie_rating  \\\n",
       "0               action fantasy war imax          0.721622   \n",
       "1                       adventure drama          0.610526   \n",
       "2                action sci-fi thriller          0.638095   \n",
       "3  adventure crime drama horror mystery          0.900000   \n",
       "4                          comedy crime          0.641667   \n",
       "\n",
       "                                     user_all_genres  \n",
       "0  fantasy sci-fi mystery animation documentary w...  \n",
       "1  fantasy sci-fi musical horror mystery western ...  \n",
       "2  sci-fi drama children thriller western film-no...  \n",
       "3  sci-fi drama children thriller western film-no...  \n",
       "4  sci-fi drama children thriller western film-no...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe41733-d5ef-499d-b8c7-2b50ba8dbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataframes to list for convenience\n",
    "df_list = [train_df, val_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be689696-94be-41a3-9376-3e68ea9cef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_movie_rating</th>\n",
       "      <th>user_all_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>51662</td>\n",
       "      <td>0.4</td>\n",
       "      <td>300 (2007)</td>\n",
       "      <td>action fantasy war imax</td>\n",
       "      <td>0.721622</td>\n",
       "      <td>fantasy sci-fi mystery animation documentary w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Robin Hood: Prince of Thieves (1991)</td>\n",
       "      <td>adventure drama</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>fantasy sci-fi musical horror mystery western ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394</td>\n",
       "      <td>45499</td>\n",
       "      <td>0.5</td>\n",
       "      <td>X-Men: The Last Stand (2006)</td>\n",
       "      <td>action sci-fi thriller</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>sci-fi drama children thriller western film-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>271</td>\n",
       "      <td>60609</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Death Note (2006)</td>\n",
       "      <td>adventure crime drama horror mystery</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>sci-fi drama children thriller western film-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489</td>\n",
       "      <td>3301</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Whole Nine Yards, The (2000)</td>\n",
       "      <td>comedy crime</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>sci-fi drama children thriller western film-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating                                 title  \\\n",
       "0     442    51662     0.4                            300 (2007)   \n",
       "1     417     1027     0.4  Robin Hood: Prince of Thieves (1991)   \n",
       "2     394    45499     0.5          X-Men: The Last Stand (2006)   \n",
       "3     271    60609     0.9                     Death Note (2006)   \n",
       "4     489     3301     0.6          Whole Nine Yards, The (2000)   \n",
       "\n",
       "                                 genres  avg_movie_rating  \\\n",
       "0               action fantasy war imax          0.721622   \n",
       "1                       adventure drama          0.610526   \n",
       "2                action sci-fi thriller          0.638095   \n",
       "3  adventure crime drama horror mystery          0.900000   \n",
       "4                          comedy crime          0.641667   \n",
       "\n",
       "                                     user_all_genres  \n",
       "0  fantasy sci-fi mystery animation documentary w...  \n",
       "1  fantasy sci-fi musical horror mystery western ...  \n",
       "2  sci-fi drama children thriller western film-no...  \n",
       "3  sci-fi drama children thriller western film-no...  \n",
       "4  sci-fi drama children thriller western film-no...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "for df in df_list:\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd2da-a0a1-4d0e-b1d4-641427d75a4c",
   "metadata": {},
   "source": [
    "### 1.2. Preprocess raw features and make Embeddings with Keras preprocessing layers.\n",
    "\n",
    "We know that raw features may not be sufficiently accessible or practically usable by machine learning models and need to be preprocessed before the data is made available for training. This process typically involves:\n",
    "- normalizing numerical features so that their impact on learning are not minimized or overpronounced relative to others.\n",
    "- turning categorical features into *embeddings*.\n",
    "- *tokenizing* textual features to translate them into embeddings.\n",
    "\n",
    "We already normalized the *rating* column by scaling down the values to fit between 0.0 and 1.0. But remember that the movie titles and genre data are still strings, so we process them in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a3950-b9d2-455d-80ad-15e4329aaabe",
   "metadata": {},
   "source": [
    "#### 1.2.1. Converting primary features into categorical data\n",
    "\n",
    "Categorical features represent discrete data. However, for our dataset, we see that the user and movie ids are of integer data types and therefore express continuous quantities. As such, we start by transforming them to the appropriate data type, which we achieved through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d2e629-6d25-4d33-a1e9-08f7b39ca95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId               object\n",
       "movieId              object\n",
       "rating              float64\n",
       "title                object\n",
       "genres               object\n",
       "avg_movie_rating    float64\n",
       "user_all_genres      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert id features into string data to allow\n",
    "# tokenization with keras\n",
    "for df in df_list:\n",
    "    id_cols = df.columns[df.columns.str.contains('Id')].tolist()\n",
    "    for col in id_cols:\n",
    "        df[col] = tf.convert_to_tensor(df[col].astype('string'), dtype=tf.string)\n",
    "\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019d450-02e8-4fa8-9a14-00f462c1305d",
   "metadata": {},
   "source": [
    "Most deep learning models express categorical data as high-dimensional embedding vectors that can be adjusted during model training. We can achieve this by building a '**vocabulary**' that maps each raw value into unique integers that can then be turned into embedding vectors.\n",
    "\n",
    "We start by making a `Keras` `StringLookup` layer for the mapping. The `StringLookup` layer is a ***non-trainable*** layer and its *state*, the vocabulary, must be constructed and set before training in a step called 'adaptation.' This includes one or more unknown - or 'out of vocabulary,' OOV - tokens that allow the layer to handle categorical values that are not in the vocabulary, and consequently, ensures that the model can continue to learn using features that have not been seen during vocabulary construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1192fbe-668c-44f1-b217-8227da1292a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '156', '359', '208', '394', '298', '116', '104', '424', '348']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a keras string lookup layer\n",
    "userId_lookup_layer = StringLookup(mask_token=None)\n",
    "movieId_lookup_layer = StringLookup(mask_token=None)\n",
    "\n",
    "for df in df_list:\n",
    "    userId_lookup_layer.adapt(df['userId'])\n",
    "    movieId_lookup_layer.adapt(df['movieId'])\n",
    "\n",
    "# verify tokenization\n",
    "userId_lookup_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667e342e-6d33-46ff-bd7e-28942faea78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(45794,), dtype=int64, numpy=array([237,  46,   4, ...,  22, 113, 291])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userId_lookup_layer(train_df['userId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd629a68-8fd0-471e-9601-3025a74591d1",
   "metadata": {},
   "source": [
    "#### 1.2.2. Tokenize textual features and translate them into embeddings\n",
    "\n",
    "Textual data is tokenized as words so we can create word embeddings to represent words as dense vectors of real numbers, where each dimension represents a different feature of the word. This allows us to capture the semantic meaning of words and their relationships to other words in way that is useful to machine learning models.\n",
    "\n",
    "In our case, tokenizing then transforming the genre data into embeddings will allow the deep network to predict movie preferences by identifying and generalizing their contextual meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8c159d5-9eab-43fb-b9da-09768ab5703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary[0:10]: ['', '[UNK]', 'thriller', 'drama', 'comedy', 'action', 'romance', 'adventure', 'crime', 'scifi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(45794, 7), dtype=int64, numpy=\n",
       "array([[ 5, 10, 12, ...,  0,  0,  0],\n",
       "       [ 7,  3,  0, ...,  0,  0,  0],\n",
       "       [ 5,  9,  2, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 5,  4,  9, ...,  0,  0,  0],\n",
       "       [ 2,  0,  0, ...,  0,  0,  0],\n",
       "       [ 4,  6,  0, ...,  0,  0,  0]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras TextVectorization layer turns raw string data into an encoded\n",
    "# representation that can be read by an embedding or dense layer\n",
    "\n",
    "# get all columns with string data\n",
    "str_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "# str_cols.remove('title')\n",
    "\n",
    "for df in df_list:\n",
    "    for col_name in str_cols:\n",
    "        vectorizer = TextVectorization()\n",
    "        vectorizer.adapt(df[col_name])\n",
    "\n",
    "# verify tokenization\n",
    "print(f'vocabulary[0:10]: {vectorizer.get_vocabulary()[:10]}')\n",
    "vectorizer(train_df['genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae853f-0a50-480a-a9d5-af12a760f4cd",
   "metadata": {},
   "source": [
    "## 2. Create the model\n",
    "\n",
    "In this step, we create a [`tf.estimator.DNNLinearCombinedClassifier`](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html) estimator object. It is important to note that according to the documentation, estimators are deprecated, and the warnings on my notebook indicate that I should switch to `Keras`, but I don't know how to do *that* yet and this is a semester project; I will try to update the model later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d9e5c-e4e1-49be-b0f2-d931e7fdf77f",
   "metadata": {},
   "source": [
    "### 2.1. Define the input columns\n",
    "\n",
    "The `get_wide_and_deep_columns()` function returns a tuple of `(wide_columns, deep_columns)` where each item represents [`tf.feature_columns`](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html).\n",
    "\n",
    "The feature inputs for the `DNNLinearCombinedClassifier` estimator are divided into those associated with the wide, linear and classifier models, and those associated with a deep neural network. The inputs to the wide model are the user and movie ID combinations that allow us to train the linear and classifier model to memorize what users watch which movies and how they rate them. These features may be brought into the model as simple categorical features hashed into a smaller number of buckets. The inclusion of the *crossed hashes* allow the model to better understand the user-movie interactions.\n",
    "\n",
    "The target does not go into either wide or deep columns, so we need to specify it because it is useful for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aedaeea-8764-4e78-8444-3f6d472cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of examples\n",
    "user_num = len(train_df['userId'].unique())\n",
    "movie_num = len(train_df['movieId'].unique())\n",
    "genre_num = len(train_df['genres'].unique())\n",
    "user_all_genres_num = len(train_df['genres'].unique())\n",
    "\n",
    "# calculate manually added embedding dimensions\n",
    "user_dim = int(round(math.pow(user_num, 1/3)))\n",
    "movie_dim = int(round(math.pow(movie_num, 1/3)))\n",
    "genre_dim = int(round(math.pow(genre_num, 1/3)))\n",
    "user_all_genres_dim = int(round(math.pow(user_all_genres_num, 1/3)))\n",
    "\n",
    "# variables to define wide and deep columns from the dataset\n",
    "LABEL_COL = 'title'\n",
    "\n",
    "CATEGORICAL_COLS = [\n",
    "    'userId',\n",
    "    'movieId'\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    'rating',\n",
    "    'avg_movie_rating'\n",
    "]\n",
    "\n",
    "TEXT_COLS = [\n",
    "    'genres',\n",
    "    'user_all_genres'\n",
    "]\n",
    "\n",
    "HASH_BUCKET_SIZES = {\n",
    "    'userId': user_num,\n",
    "    'movieId': movie_num,\n",
    "    'genres': genre_num,\n",
    "    'user_all_genres': user_all_genres_num\n",
    "}\n",
    "\n",
    "EMBEDDING_DIMENSIONS = {\n",
    "    'userId': user_dim,\n",
    "    'movieId': movie_dim,\n",
    "    'genres': genre_dim,\n",
    "    'user_all_genres': user_all_genres_dim,\n",
    "}\n",
    "\n",
    "# define wide and deep columns\n",
    "def get_wide_and_deep_columns():\n",
    "    wide_cols, deep_cols = [], []\n",
    "    text_buckets = []\n",
    "    numeric_cols, numeric_buckets = [], []\n",
    "    cat_hash_bucket_size = genre_num * genre_dim\n",
    "    l, r = 3/5, 4.5/5\n",
    "\n",
    "    # categorical embedding columns\n",
    "    for col_name in CATEGORICAL_COLS:\n",
    "        categorical_col = tf.feature_column.categorical_column_with_identity(\n",
    "            col_name,\n",
    "            num_buckets = HASH_BUCKET_SIZES[col_name])\n",
    "        wrapped_col = tf.feature_column.embedding_column(\n",
    "            categorical_col,\n",
    "            dimension = EMBEDDING_DIMENSIONS[col_name],\n",
    "            combiner = 'sqrtn')\n",
    "        wide_cols.append(categorical_col)\n",
    "        deep_cols.append(wrapped_col)\n",
    "\n",
    "    # text data embedding\n",
    "    for col_name in TEXT_COLS:\n",
    "        text_col = tf.feature_column.categorical_column_with_identity(\n",
    "            col_name,\n",
    "            num_buckets = HASH_BUCKET_SIZES[col_name])\n",
    "        wrapped_col = tf.feature_column.embedding_column(\n",
    "            categorical_col,\n",
    "            dimension = EMBEDDING_DIMENSIONS[col_name],\n",
    "            combiner = 'sqrtn')\n",
    "        text_buckets.append(col_name)\n",
    "        wide_cols.append(text_col)\n",
    "        deep_cols.append(wrapped_col)\n",
    "\n",
    "    # numeric columns\n",
    "    for col_name in NUMERIC_COLS:\n",
    "        col_name = tf.feature_column.numeric_column(\n",
    "            col_name,\n",
    "            shape = (1,),\n",
    "            dtype = tf.float32)\n",
    "        col_buckets = tf.feature_column.bucketized_column(\n",
    "            col_name,\n",
    "            boundaries=[l, r])\n",
    "        numeric_cols.append(col_name)\n",
    "        numeric_buckets.append(col_buckets)\n",
    "        deep_cols.append(col_name)\n",
    "\n",
    "    # cross numeric columns, text data columns\n",
    "    numeric_cols_crossed = tf.feature_column.crossed_column(numeric_buckets, 12)\n",
    "    text_cols_crossed = tf.feature_column.crossed_column(text_buckets, cat_hash_bucket_size)\n",
    "\n",
    "    # add buckets and crossed columns to set of wide columns\n",
    "    wide_cols.extend([numeric_buckets, numeric_cols_crossed, text_cols_crossed])\n",
    "\n",
    "    return wide_cols, deep_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918af01-2091-47ca-84b4-33170dc26ea9",
   "metadata": {},
   "source": [
    "As we pointed out earlier, embeddings allow machine learning models to perform better. One vital property of embedding is that are semantically similar are spacially closer to each other, which means they have similar vector representations as measured by a distance metric such as *cosine similarity*.\n",
    "\n",
    "The **dimensionality** of the word embeddings refers to the number of dimensions in which the vector representation of the word is defined, that is, the total number of features that are encoded in the word. The number of dimensions can have a significant impact on the performance: too few and the embeddings might not capture enough relevant information about the data; too many and the number of embeddings might become too complex, causing the model to overfit, and consequently, leading to poor generalization of new data. Setting the dimensionality is conventionally done through trial and error, much like adjusting the weights and biases of a model to improve its performance. Here we use the third root of the number of unique items in each column.\n",
    "\n",
    "**Hash bucket sizes** are used to represent categorical data as a fixed-length vector of integers by hashing the categorical values into a fixed number of buckets, which are then used as indices into the vector. By using a fixed number of buckets, we can represent a large number of categories with a relatively small number of dimensions. This can help to reduce overfitting and improve the generalization performance of machine learning models. The optimal hash bucket size depends on the specific dataset and task. In general, it is recommended to use a hash bucket size that is large enough to capture all the relevant information in the data, but not so large that it becomes computationally expensive to train the model.\n",
    "\n",
    "> **TODO**: find out how exactly the dimensions of the wide-and-deep model are similar/differ from the dimensions of the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f41d238e-4661-4548-8338-312408a408d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns, deep_columns = get_wide_and_deep_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9293817-880e-48de-8de2-fffe80b3cee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IdentityCategoricalColumn(key='userId', number_buckets=500, default_value=None),\n",
       " IdentityCategoricalColumn(key='movieId', number_buckets=6368, default_value=None),\n",
       " IdentityCategoricalColumn(key='genres', number_buckets=766, default_value=None),\n",
       " IdentityCategoricalColumn(key='user_all_genres', number_buckets=766, default_value=None),\n",
       " [BucketizedColumn(source_column=NumericColumn(key='rating', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.6, 0.9)),\n",
       "  BucketizedColumn(source_column=NumericColumn(key='avg_movie_rating', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.6, 0.9))],\n",
       " CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='rating', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.6, 0.9)), BucketizedColumn(source_column=NumericColumn(key='avg_movie_rating', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.6, 0.9))), hash_bucket_size=12, hash_key=None),\n",
       " CrossedColumn(keys=('genres', 'user_all_genres'), hash_bucket_size=6894, hash_key=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b07ffb0-800c-4c50-9fc6-c8184b00144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='userId', number_buckets=500, default_value=None), dimension=8, combiner='sqrtn', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f2174b3b7c0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='movieId', number_buckets=6368, default_value=None), dimension=19, combiner='sqrtn', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f2174b3b5e0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='movieId', number_buckets=6368, default_value=None), dimension=9, combiner='sqrtn', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f2174b3be80>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='movieId', number_buckets=6368, default_value=None), dimension=9, combiner='sqrtn', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f2174b3b550>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " NumericColumn(key='rating', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='avg_movie_rating', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83406d3a-570b-4dc5-969b-63472ec67d61",
   "metadata": {},
   "source": [
    "### 1.2. Define the wide-and-deep model\n",
    "\n",
    "We are ready to assemble our estimator. We use the `Ftrl` and `Adagrad` optimizers that have their default learning rate and other parameters that we can modify later, then declare the number of layers for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf50097c-6999-41d5-ba89-3a964d25e376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/model_checkpoint', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier\n",
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    # wide settings\n",
    "    linear_feature_columns=wide_columns,\n",
    "    linear_optimizer=tf.keras.optimizers.Ftrl(),\n",
    "\n",
    "    # deep settings\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50],\n",
    "    dnn_optimizer=tf.keras.optimizers.Adagrad(),\n",
    "\n",
    "    # warm-start settings\n",
    "    model_dir=CHECKPOINT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a148f-c8c5-4f7a-bd77-8f2788dc697b",
   "metadata": {},
   "source": [
    "### 1.3. Create the custom metric\n",
    "\n",
    "We learn from this [Databricks](https://notebooks.databricks.com/notebooks/RCG/Wide_and_Deep/index.html#Wide_and_Deep_3.html) tutorial that for recommenders where the goal is to present items in order from most to least likely to be selected, *average precision at k* is conventionally used. Ultimately, our goal is to recommend video games and tournaments in this very fashion, so this metric works for us.\n",
    "\n",
    "> the metric examines the average precision associated with a top-k number of recommendations. The closer the value of MAP@K (the average precision at k), the better aligned those recommendations are with a customer's product selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5548bf62-674f-4247-93d6-c160d42a3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/Recommendation/WideAndDeep/utils/metrics.py\n",
    "def map_custom_metric(features, labels, predictions):\n",
    "    user_ids = tf.reshape(features['userId'], [-1])\n",
    "    predictions = predictions['probabilities'][:, 1]\n",
    "    \n",
    "    # Processing unique userIds, indices and counts\n",
    "    # Sorting needed in case the same userId occurs in two different places\n",
    "    sorted_ids = tf.argsort(user_ids)\n",
    "    user_ids = tf.gather(user_ids, indices=sorted_ids)\n",
    "    predictions = tf.gather(predictions, indices=sorted_ids)\n",
    "    labels = tf.gather(labels, indices=sorted_ids)\n",
    "    \n",
    "    _, user_ids_idx, user_ids_movies_count = tf.unique_with_counts(user_ids, out_idx=tf.int64)\n",
    "    pad_length = 30 - tf.reduce_max(user_ids_movies_count)\n",
    "    pad_fn = lambda x: tf.pad(x, [(0, 0), (0, pad_length)])\n",
    "    \n",
    "    preds = tf.RaggedTensor.from_value_rowids(predictions, user_ids_idx).to_tensor()\n",
    "    labels = tf.RaggedTensor.from_value_rowids(labels, user_ids_idx).to_tensor()\n",
    "    \n",
    "    labels = tf.argmax(labels, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'map': tf.compat.v1.metrics.average_precision_at_k(\n",
    "            predictions=pad_fn(preds),\n",
    "            labels=labels,\n",
    "            k=5,\n",
    "            name=\"streaming_map\"\n",
    "        )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "232c5fc7-f786-4297-a7d7-83f02adcb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_21523/1423564771.py:1: add_metrics (from tensorflow_estimator.python.estimator.extenders) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/model_checkpoint', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.add_metrics(estimator, map_custom_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118df69-357d-4f57-b34b-8ebbe5d082c4",
   "metadata": {},
   "source": [
    "## 3. Train and evaluate the model\n",
    "\n",
    "We start by aggregating all the preprocessed columns into a `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb1085d-039b-43b6-8f27-9aa1cc919a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the numerical and vectorized data to create a\n",
    "# tensorflow dataset\n",
    "\n",
    "tfd_list = ['train', 'test', 'val']\n",
    "i = 0\n",
    "\n",
    "for df in df_list:\n",
    "    userId = userId_lookup_layer(df['userId'])\n",
    "    movieId = userId_lookup_layer(df['movieId'])\n",
    "    rating = df['rating']\n",
    "    avg_movie_rating = df['avg_movie_rating']\n",
    "    genres = vectorizer(df['genres'])\n",
    "    user_all_genres = vectorizer(df['user_all_genres'])\n",
    "    title = vectorizer(df['title'])\n",
    "    \n",
    "    skibidi = (userId, movieId, rating, avg_movie_rating, genres, user_all_genres, title)\n",
    "    \n",
    "    globals()[f'{tfd_list[i]}_tf_dataset'] = tf.data.Dataset.from_tensor_slices(skibidi)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af963b93-1a42-43cc-8499-00d753c782bd",
   "metadata": {},
   "source": [
    "Finally, we connect the model to the training data using an input function. The function creates the tensorflow operations that generate data for the model. It maps the raw (preprocessed) data into the model.\n",
    "\n",
    "We found that we need to make the `tf.data.Dataset` iterable before we can train the model with it, and one way to do this was by using `as_numpy_iterator()`. However, it appears that this can only happen when running in ***eager execution*** mode, but unfortunately, we still could not train it after setting it as required. I am guessing that this is due to incompatibility issues with previous versions of tensorflow. What I do know is that we need to get the `numpy` data from the `TensorSliceDataset` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da50569-1fd1-44cb-9eb6-0546eedbc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function that converts the tf.data.Dataset into a tuple\n",
    "def to_tuple(batch):\n",
    "    iterator = batch.as_numpy_iterator()\n",
    "    for item in iterator:\n",
    "        features = {\n",
    "            'userId': item[0],\n",
    "            'movieId': item[1],\n",
    "            'rating': item[2],\n",
    "            'avg_movie_rating': item[3],\n",
    "            'genres': item[4],\n",
    "            'user_all_genres': item[5]\n",
    "        }\n",
    "        label = item[6]\n",
    "    return features, label\n",
    "\n",
    "# utility function that creates the input function from the tf dataset\n",
    "def get_input_fn(dataset_context_manager):\n",
    "    def _fn():\n",
    "        return to_tuple(dataset_context_manager)\n",
    "    return _fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92d107-a5ea-4ecd-9bc7-90f3c88ea42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=lambda: get_input_fn(train_tf_dataset), max_steps=29000)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=lambda: get_input_fn(val_tf_dataset))\n",
    "\n",
    "with mlflow.start_run():\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    artifact_uri = mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31078060-9505-42b1-a18e-c2df4d9aca1f",
   "metadata": {},
   "source": [
    "## 4. Future\n",
    "\n",
    "A high priority is the backwards compatibility issues and learning to adapt Keras for the model so that it can be trained and tested better. This will provide a solid foundation for the recommendation engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
